---
title: Sycophantic AI increases attitude extremity and overconfidence
publication_types:
  - "0"
authors:
  - Steve Rathje
  - Meryl Ye
  - Laura K. Globig
  - Raunak M. Pillai
  - Victoria Oldemburgo de Mello
  - Jay J. Van Bavel
doi: https://doi.org/10.31234/osf.io/vmyek_v1
publication: OSF Preprint
abstract: AI chatbots have been shown to be successful tools for persuasion.
  However, people may prefer to use chatbots that validate, rather than
  challenge, their pre-existing beliefs. This preference for “sycophantic” (or
  overly agreeable and validating) chatbots may entrench beliefs and make it
  challenging to deploy AI systems that open people up to new perspectives.
  Across three experiments (n = 3,285) involving four political topics and four
  large language models, we found that people consistently preferred and chose
  to interact with sycophantic AI models over disagreeable chatbots that
  challenged their beliefs. Brief conversations with sycophantic chatbots
  increased attitude extremity and certainty, whereas disagreeable chatbots
  decreased attitude extremity and certainty. Sycophantic chatbots also inflated
  people’s perception that they are “better than average” on a number of
  desirable traits (e.g., intelligence, empathy). Furthermore, people viewed
  sycophantic chatbots as unbiased, but viewed disagreeable chatbots as highly
  biased. Sycophantic chatbots’ impact on attitude extremity and certainty was
  driven by a one-sided presentation of facts, whereas their impact on enjoyment
  was driven by validation. Altogether, these results suggest that people’s
  preference for and blindness to sycophantic AI may risk creating AI “echo
  chambers” that increase attitude extremity and overconfidence.
draft: false
featured: true
image:
  filename: featured
  focal_point: Smart
  preview_only: false
date: 2025-11-12T17:38:52.864Z
---
